{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/MACCROBAT2018.zip' -d '/content/MACCROBAT2018'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "poh34v0FlRXR",
        "outputId": "e72dd870-763b-4d77-bb8a-4d13f3ba2ef6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/MACCROBAT2018.zip\n",
            "  inflating: /content/MACCROBAT2018/15939911.ann  \n",
            "  inflating: /content/MACCROBAT2018/15939911.txt  \n",
            "  inflating: /content/MACCROBAT2018/16778410.ann  \n",
            "  inflating: /content/MACCROBAT2018/16778410.txt  \n",
            "  inflating: /content/MACCROBAT2018/17803823.ann  \n",
            "  inflating: /content/MACCROBAT2018/17803823.txt  \n",
            "  inflating: /content/MACCROBAT2018/18236639.ann  \n",
            "  inflating: /content/MACCROBAT2018/18236639.txt  \n",
            "  inflating: /content/MACCROBAT2018/18258107.ann  \n",
            "  inflating: /content/MACCROBAT2018/18258107.txt  \n",
            "  inflating: /content/MACCROBAT2018/18416479.ann  \n",
            "  inflating: /content/MACCROBAT2018/18416479.txt  \n",
            "  inflating: /content/MACCROBAT2018/18561524.ann  \n",
            "  inflating: /content/MACCROBAT2018/18561524.txt  \n",
            "  inflating: /content/MACCROBAT2018/18666334.ann  \n",
            "  inflating: /content/MACCROBAT2018/18666334.txt  \n",
            "  inflating: /content/MACCROBAT2018/18787726.ann  \n",
            "  inflating: /content/MACCROBAT2018/18787726.txt  \n",
            "  inflating: /content/MACCROBAT2018/18815636.ann  \n",
            "  inflating: /content/MACCROBAT2018/18815636.txt  \n",
            "  inflating: /content/MACCROBAT2018/19009665.ann  \n",
            "  inflating: /content/MACCROBAT2018/19009665.txt  \n",
            "  inflating: /content/MACCROBAT2018/19214295.ann  \n",
            "  inflating: /content/MACCROBAT2018/19214295.txt  \n",
            "  inflating: /content/MACCROBAT2018/19307547.ann  \n",
            "  inflating: /content/MACCROBAT2018/19307547.txt  \n",
            "  inflating: /content/MACCROBAT2018/19610147.ann  \n",
            "  inflating: /content/MACCROBAT2018/19610147.txt  \n",
            "  inflating: /content/MACCROBAT2018/19816630.ann  \n",
            "  inflating: /content/MACCROBAT2018/19816630.txt  \n",
            "  inflating: /content/MACCROBAT2018/19860006.ann  \n",
            "  inflating: /content/MACCROBAT2018/19860006.txt  \n",
            "  inflating: /content/MACCROBAT2018/19860007.ann  \n",
            "  inflating: /content/MACCROBAT2018/19860007.txt  \n",
            "  inflating: /content/MACCROBAT2018/19860925.ann  \n",
            "  inflating: /content/MACCROBAT2018/19860925.txt  \n",
            "  inflating: /content/MACCROBAT2018/20146086.ann  \n",
            "  inflating: /content/MACCROBAT2018/20146086.txt  \n",
            "  inflating: /content/MACCROBAT2018/20671919.ann  \n",
            "  inflating: /content/MACCROBAT2018/20671919.txt  \n",
            "  inflating: /content/MACCROBAT2018/20977862.ann  \n",
            "  inflating: /content/MACCROBAT2018/20977862.txt  \n",
            "  inflating: /content/MACCROBAT2018/21067996.ann  \n",
            "  inflating: /content/MACCROBAT2018/21067996.txt  \n",
            "  inflating: /content/MACCROBAT2018/21129213.ann  \n",
            "  inflating: /content/MACCROBAT2018/21129213.txt  \n",
            "  inflating: /content/MACCROBAT2018/21254744.ann  \n",
            "  inflating: /content/MACCROBAT2018/21254744.txt  \n",
            "  inflating: /content/MACCROBAT2018/21308977.ann  \n",
            "  inflating: /content/MACCROBAT2018/21308977.txt  \n",
            "  inflating: /content/MACCROBAT2018/21477357.ann  \n",
            "  inflating: /content/MACCROBAT2018/21477357.txt  \n",
            "  inflating: /content/MACCROBAT2018/21505579.ann  \n",
            "  inflating: /content/MACCROBAT2018/21505579.txt  \n",
            "  inflating: /content/MACCROBAT2018/21527041.ann  \n",
            "  inflating: /content/MACCROBAT2018/21527041.txt  \n",
            "  inflating: /content/MACCROBAT2018/21672201.ann  \n",
            "  inflating: /content/MACCROBAT2018/21672201.txt  \n",
            "  inflating: /content/MACCROBAT2018/21720478.ann  \n",
            "  inflating: /content/MACCROBAT2018/21720478.txt  \n",
            "  inflating: /content/MACCROBAT2018/21923918.ann  \n",
            "  inflating: /content/MACCROBAT2018/21923918.txt  \n",
            "  inflating: /content/MACCROBAT2018/22218279.ann  \n",
            "  inflating: /content/MACCROBAT2018/22218279.txt  \n",
            "  inflating: /content/MACCROBAT2018/22514576.ann  \n",
            "  inflating: /content/MACCROBAT2018/22514576.txt  \n",
            "  inflating: /content/MACCROBAT2018/22515939.ann  \n",
            "  inflating: /content/MACCROBAT2018/22515939.txt  \n",
            "  inflating: /content/MACCROBAT2018/22520024.ann  \n",
            "  inflating: /content/MACCROBAT2018/22520024.txt  \n",
            "  inflating: /content/MACCROBAT2018/22665582.ann  \n",
            "  inflating: /content/MACCROBAT2018/22665582.txt  \n",
            "  inflating: /content/MACCROBAT2018/22719160.ann  \n",
            "  inflating: /content/MACCROBAT2018/22719160.txt  \n",
            "  inflating: /content/MACCROBAT2018/22781096.ann  \n",
            "  inflating: /content/MACCROBAT2018/22781096.txt  \n",
            "  inflating: /content/MACCROBAT2018/22791498.ann  \n",
            "  inflating: /content/MACCROBAT2018/22791498.txt  \n",
            "  inflating: /content/MACCROBAT2018/22814979.ann  \n",
            "  inflating: /content/MACCROBAT2018/22814979.txt  \n",
            "  inflating: /content/MACCROBAT2018/23033875.ann  \n",
            "  inflating: /content/MACCROBAT2018/23033875.txt  \n",
            "  inflating: /content/MACCROBAT2018/23035161.ann  \n",
            "  inflating: /content/MACCROBAT2018/23035161.txt  \n",
            "  inflating: /content/MACCROBAT2018/23076693.ann  \n",
            "  inflating: /content/MACCROBAT2018/23076693.txt  \n",
            "  inflating: /content/MACCROBAT2018/23077697.ann  \n",
            "  inflating: /content/MACCROBAT2018/23077697.txt  \n",
            "  inflating: /content/MACCROBAT2018/23124805.ann  \n",
            "  inflating: /content/MACCROBAT2018/23124805.txt  \n",
            "  inflating: /content/MACCROBAT2018/23155491.ann  \n",
            "  inflating: /content/MACCROBAT2018/23155491.txt  \n",
            "  inflating: /content/MACCROBAT2018/23242090.ann  \n",
            "  inflating: /content/MACCROBAT2018/23242090.txt  \n",
            "  inflating: /content/MACCROBAT2018/23312850.ann  \n",
            "  inflating: /content/MACCROBAT2018/23312850.txt  \n",
            "  inflating: /content/MACCROBAT2018/23468586.ann  \n",
            "  inflating: /content/MACCROBAT2018/23468586.txt  \n",
            "  inflating: /content/MACCROBAT2018/23678274.ann  \n",
            "  inflating: /content/MACCROBAT2018/23678274.txt  \n",
            "  inflating: /content/MACCROBAT2018/23864579.ann  \n",
            "  inflating: /content/MACCROBAT2018/23864579.txt  \n",
            "  inflating: /content/MACCROBAT2018/23897372.ann  \n",
            "  inflating: /content/MACCROBAT2018/23897372.txt  \n",
            "  inflating: /content/MACCROBAT2018/24043987.ann  \n",
            "  inflating: /content/MACCROBAT2018/24043987.txt  \n",
            "  inflating: /content/MACCROBAT2018/24161539.ann  \n",
            "  inflating: /content/MACCROBAT2018/24161539.txt  \n",
            "  inflating: /content/MACCROBAT2018/24294397.ann  \n",
            "  inflating: /content/MACCROBAT2018/24294397.txt  \n",
            "  inflating: /content/MACCROBAT2018/24518095.ann  \n",
            "  inflating: /content/MACCROBAT2018/24518095.txt  \n",
            "  inflating: /content/MACCROBAT2018/24526194.ann  \n",
            "  inflating: /content/MACCROBAT2018/24526194.txt  \n",
            "  inflating: /content/MACCROBAT2018/24654246.ann  \n",
            "  inflating: /content/MACCROBAT2018/24654246.txt  \n",
            "  inflating: /content/MACCROBAT2018/24781756.ann  \n",
            "  inflating: /content/MACCROBAT2018/24781756.txt  \n",
            "  inflating: /content/MACCROBAT2018/24898994.ann  \n",
            "  inflating: /content/MACCROBAT2018/24898994.txt  \n",
            "  inflating: /content/MACCROBAT2018/24957905.ann  \n",
            "  inflating: /content/MACCROBAT2018/24957905.txt  \n",
            "  inflating: /content/MACCROBAT2018/25023062.ann  \n",
            "  inflating: /content/MACCROBAT2018/25023062.txt  \n",
            "  inflating: /content/MACCROBAT2018/25024632.ann  \n",
            "  inflating: /content/MACCROBAT2018/25024632.txt  \n",
            "  inflating: /content/MACCROBAT2018/25139918.ann  \n",
            "  inflating: /content/MACCROBAT2018/25139918.txt  \n",
            "  inflating: /content/MACCROBAT2018/25155594.ann  \n",
            "  inflating: /content/MACCROBAT2018/25155594.txt  \n",
            "  inflating: /content/MACCROBAT2018/25210224.ann  \n",
            "  inflating: /content/MACCROBAT2018/25210224.txt  \n",
            "  inflating: /content/MACCROBAT2018/25246819.ann  \n",
            "  inflating: /content/MACCROBAT2018/25246819.txt  \n",
            "  inflating: /content/MACCROBAT2018/25293719.ann  \n",
            "  inflating: /content/MACCROBAT2018/25293719.txt  \n",
            "  inflating: /content/MACCROBAT2018/25295501.ann  \n",
            "  inflating: /content/MACCROBAT2018/25295501.txt  \n",
            "  inflating: /content/MACCROBAT2018/25370695.ann  \n",
            "  inflating: /content/MACCROBAT2018/25370695.txt  \n",
            "  inflating: /content/MACCROBAT2018/25410034.ann  \n",
            "  inflating: /content/MACCROBAT2018/25410034.txt  \n",
            "  inflating: /content/MACCROBAT2018/25410883.ann  \n",
            "  inflating: /content/MACCROBAT2018/25410883.txt  \n",
            "  inflating: /content/MACCROBAT2018/25572898.ann  \n",
            "  inflating: /content/MACCROBAT2018/25572898.txt  \n",
            "  inflating: /content/MACCROBAT2018/25661749.ann  \n",
            "  inflating: /content/MACCROBAT2018/25661749.txt  \n",
            "  inflating: /content/MACCROBAT2018/25721834.ann  \n",
            "  inflating: /content/MACCROBAT2018/25721834.txt  \n",
            "  inflating: /content/MACCROBAT2018/25743872.ann  \n",
            "  inflating: /content/MACCROBAT2018/25743872.txt  \n",
            "  inflating: /content/MACCROBAT2018/25759562.ann  \n",
            "  inflating: /content/MACCROBAT2018/25759562.txt  \n",
            "  inflating: /content/MACCROBAT2018/25793030.ann  \n",
            "  inflating: /content/MACCROBAT2018/25793030.txt  \n",
            "  inflating: /content/MACCROBAT2018/25853982.ann  \n",
            "  inflating: /content/MACCROBAT2018/25853982.txt  \n",
            "  inflating: /content/MACCROBAT2018/25858931.ann  \n",
            "  inflating: /content/MACCROBAT2018/25858931.txt  \n",
            "  inflating: /content/MACCROBAT2018/25884600.ann  \n",
            "  inflating: /content/MACCROBAT2018/25884600.txt  \n",
            "  inflating: /content/MACCROBAT2018/25926582.ann  \n",
            "  inflating: /content/MACCROBAT2018/25926582.txt  \n",
            "  inflating: /content/MACCROBAT2018/25934795.ann  \n",
            "  inflating: /content/MACCROBAT2018/25934795.txt  \n",
            "  inflating: /content/MACCROBAT2018/26106249.ann  \n",
            "  inflating: /content/MACCROBAT2018/26106249.txt  \n",
            "  inflating: /content/MACCROBAT2018/26175648.ann  \n",
            "  inflating: /content/MACCROBAT2018/26175648.txt  \n",
            "  inflating: /content/MACCROBAT2018/26216058.ann  \n",
            "  inflating: /content/MACCROBAT2018/26216058.txt  \n",
            "  inflating: /content/MACCROBAT2018/26228535.ann  \n",
            "  inflating: /content/MACCROBAT2018/26228535.txt  \n",
            "  inflating: /content/MACCROBAT2018/26257516.ann  \n",
            "  inflating: /content/MACCROBAT2018/26257516.txt  \n",
            "  inflating: /content/MACCROBAT2018/26264228.ann  \n",
            "  inflating: /content/MACCROBAT2018/26264228.txt  \n",
            "  inflating: /content/MACCROBAT2018/26266396.ann  \n",
            "  inflating: /content/MACCROBAT2018/26266396.txt  \n",
            "  inflating: /content/MACCROBAT2018/26285706.ann  \n",
            "  inflating: /content/MACCROBAT2018/26285706.txt  \n",
            "  inflating: /content/MACCROBAT2018/26309459.ann  \n",
            "  inflating: /content/MACCROBAT2018/26309459.txt  \n",
            "  inflating: /content/MACCROBAT2018/26313770.ann  \n",
            "  inflating: /content/MACCROBAT2018/26313770.txt  \n",
            "  inflating: /content/MACCROBAT2018/26327988.ann  \n",
            "  inflating: /content/MACCROBAT2018/26327988.txt  \n",
            "  inflating: /content/MACCROBAT2018/26336183.ann  \n",
            "  inflating: /content/MACCROBAT2018/26336183.txt  \n",
            "  inflating: /content/MACCROBAT2018/26350418.ann  \n",
            "  inflating: /content/MACCROBAT2018/26350418.txt  \n",
            "  inflating: /content/MACCROBAT2018/26361431.ann  \n",
            "  inflating: /content/MACCROBAT2018/26361431.txt  \n",
            "  inflating: /content/MACCROBAT2018/26361640.ann  \n",
            "  inflating: /content/MACCROBAT2018/26361640.txt  \n",
            "  inflating: /content/MACCROBAT2018/26395443.ann  \n",
            "  inflating: /content/MACCROBAT2018/26395443.txt  \n",
            "  inflating: /content/MACCROBAT2018/26405496.ann  \n",
            "  inflating: /content/MACCROBAT2018/26405496.txt  \n",
            "  inflating: /content/MACCROBAT2018/26444414.ann  \n",
            "  inflating: /content/MACCROBAT2018/26444414.txt  \n",
            "  inflating: /content/MACCROBAT2018/26445413.ann  \n",
            "  inflating: /content/MACCROBAT2018/26445413.txt  \n",
            "  inflating: /content/MACCROBAT2018/26457578.ann  \n",
            "  inflating: /content/MACCROBAT2018/26457578.txt  \n",
            "  inflating: /content/MACCROBAT2018/26469535.ann  \n",
            "  inflating: /content/MACCROBAT2018/26469535.txt  \n",
            "  inflating: /content/MACCROBAT2018/26474553.ann  \n",
            "  inflating: /content/MACCROBAT2018/26474553.txt  \n",
            "  inflating: /content/MACCROBAT2018/26523273.ann  \n",
            "  inflating: /content/MACCROBAT2018/26523273.txt  \n",
            "  inflating: /content/MACCROBAT2018/26530965.ann  \n",
            "  inflating: /content/MACCROBAT2018/26530965.txt  \n",
            "  inflating: /content/MACCROBAT2018/26584481.ann  \n",
            "  inflating: /content/MACCROBAT2018/26584481.txt  \n",
            "  inflating: /content/MACCROBAT2018/26629302.ann  \n",
            "  inflating: /content/MACCROBAT2018/26629302.txt  \n",
            "  inflating: /content/MACCROBAT2018/26656340.ann  \n",
            "  inflating: /content/MACCROBAT2018/26656340.txt  \n",
            "  inflating: /content/MACCROBAT2018/26664317.ann  \n",
            "  inflating: /content/MACCROBAT2018/26664317.txt  \n",
            "  inflating: /content/MACCROBAT2018/26670309.ann  \n",
            "  inflating: /content/MACCROBAT2018/26670309.txt  \n",
            "  inflating: /content/MACCROBAT2018/26675562.ann  \n",
            "  inflating: /content/MACCROBAT2018/26675562.txt  \n",
            "  inflating: /content/MACCROBAT2018/26683938.ann  \n",
            "  inflating: /content/MACCROBAT2018/26683938.txt  \n",
            "  inflating: /content/MACCROBAT2018/26692730.ann  \n",
            "  inflating: /content/MACCROBAT2018/26692730.txt  \n",
            "  inflating: /content/MACCROBAT2018/26714786.ann  \n",
            "  inflating: /content/MACCROBAT2018/26714786.txt  \n",
            "  inflating: /content/MACCROBAT2018/27004009.ann  \n",
            "  inflating: /content/MACCROBAT2018/27004009.txt  \n",
            "  inflating: /content/MACCROBAT2018/27057898.ann  \n",
            "  inflating: /content/MACCROBAT2018/27057898.txt  \n",
            "  inflating: /content/MACCROBAT2018/27059701.ann  \n",
            "  inflating: /content/MACCROBAT2018/27059701.txt  \n",
            "  inflating: /content/MACCROBAT2018/27064109.ann  \n",
            "  inflating: /content/MACCROBAT2018/27064109.txt  \n",
            "  inflating: /content/MACCROBAT2018/27100441.ann  \n",
            "  inflating: /content/MACCROBAT2018/27100441.txt  \n",
            "  inflating: /content/MACCROBAT2018/27130218.ann  \n",
            "  inflating: /content/MACCROBAT2018/27130218.txt  \n",
            "  inflating: /content/MACCROBAT2018/27196481.ann  \n",
            "  inflating: /content/MACCROBAT2018/27196481.txt  \n",
            "  inflating: /content/MACCROBAT2018/27218632.ann  \n",
            "  inflating: /content/MACCROBAT2018/27218632.txt  \n",
            "  inflating: /content/MACCROBAT2018/27661040.ann  \n",
            "  inflating: /content/MACCROBAT2018/27661040.txt  \n",
            "  inflating: /content/MACCROBAT2018/27683825.ann  \n",
            "  inflating: /content/MACCROBAT2018/27683825.txt  \n",
            "  inflating: /content/MACCROBAT2018/27741115.ann  \n",
            "  inflating: /content/MACCROBAT2018/27741115.txt  \n",
            "  inflating: /content/MACCROBAT2018/27749582.ann  \n",
            "  inflating: /content/MACCROBAT2018/27749582.txt  \n",
            "  inflating: /content/MACCROBAT2018/27773410.ann  \n",
            "  inflating: /content/MACCROBAT2018/27773410.txt  \n",
            "  inflating: /content/MACCROBAT2018/27793101.ann  \n",
            "  inflating: /content/MACCROBAT2018/27793101.txt  \n",
            "  inflating: /content/MACCROBAT2018/27821134.ann  \n",
            "  inflating: /content/MACCROBAT2018/27821134.txt  \n",
            "  inflating: /content/MACCROBAT2018/27842595.ann  \n",
            "  inflating: /content/MACCROBAT2018/27842595.txt  \n",
            "  inflating: /content/MACCROBAT2018/27842605.ann  \n",
            "  inflating: /content/MACCROBAT2018/27842605.txt  \n",
            "  inflating: /content/MACCROBAT2018/27846860.ann  \n",
            "  inflating: /content/MACCROBAT2018/27846860.txt  \n",
            "  inflating: /content/MACCROBAT2018/27904130.ann  \n",
            "  inflating: /content/MACCROBAT2018/27904130.txt  \n",
            "  inflating: /content/MACCROBAT2018/27906105.ann  \n",
            "  inflating: /content/MACCROBAT2018/27906105.txt  \n",
            "  inflating: /content/MACCROBAT2018/27928148.ann  \n",
            "  inflating: /content/MACCROBAT2018/27928148.txt  \n",
            "  inflating: /content/MACCROBAT2018/27974938.ann  \n",
            "  inflating: /content/MACCROBAT2018/27974938.txt  \n",
            "  inflating: /content/MACCROBAT2018/27980261.ann  \n",
            "  inflating: /content/MACCROBAT2018/27980261.txt  \n",
            "  inflating: /content/MACCROBAT2018/27980272.ann  \n",
            "  inflating: /content/MACCROBAT2018/27980272.txt  \n",
            "  inflating: /content/MACCROBAT2018/27990013.ann  \n",
            "  inflating: /content/MACCROBAT2018/27990013.txt  \n",
            "  inflating: /content/MACCROBAT2018/27998312.ann  \n",
            "  inflating: /content/MACCROBAT2018/27998312.txt  \n",
            "  inflating: /content/MACCROBAT2018/28033278.ann  \n",
            "  inflating: /content/MACCROBAT2018/28033278.txt  \n",
            "  inflating: /content/MACCROBAT2018/28057913.ann  \n",
            "  inflating: /content/MACCROBAT2018/28057913.txt  \n",
            "  inflating: /content/MACCROBAT2018/28079821.ann  \n",
            "  inflating: /content/MACCROBAT2018/28079821.txt  \n",
            "  inflating: /content/MACCROBAT2018/28090049.ann  \n",
            "  inflating: /content/MACCROBAT2018/28090049.txt  \n",
            "  inflating: /content/MACCROBAT2018/28100235.ann  \n",
            "  inflating: /content/MACCROBAT2018/28100235.txt  \n",
            "  inflating: /content/MACCROBAT2018/28100279.ann  \n",
            "  inflating: /content/MACCROBAT2018/28100279.txt  \n",
            "  inflating: /content/MACCROBAT2018/28103924.ann  \n",
            "  inflating: /content/MACCROBAT2018/28103924.txt  \n",
            "  inflating: /content/MACCROBAT2018/28115731.ann  \n",
            "  inflating: /content/MACCROBAT2018/28115731.txt  \n",
            "  inflating: /content/MACCROBAT2018/28120581.ann  \n",
            "  inflating: /content/MACCROBAT2018/28120581.txt  \n",
            "  inflating: /content/MACCROBAT2018/28121940.ann  \n",
            "  inflating: /content/MACCROBAT2018/28121940.txt  \n",
            "  inflating: /content/MACCROBAT2018/28151860.ann  \n",
            "  inflating: /content/MACCROBAT2018/28151860.txt  \n",
            "  inflating: /content/MACCROBAT2018/28151882.ann  \n",
            "  inflating: /content/MACCROBAT2018/28151882.txt  \n",
            "  inflating: /content/MACCROBAT2018/28151916.ann  \n",
            "  inflating: /content/MACCROBAT2018/28151916.txt  \n",
            "  inflating: /content/MACCROBAT2018/28154281.ann  \n",
            "  inflating: /content/MACCROBAT2018/28154281.txt  \n",
            "  inflating: /content/MACCROBAT2018/28154287.ann  \n",
            "  inflating: /content/MACCROBAT2018/28154287.txt  \n",
            "  inflating: /content/MACCROBAT2018/28154669.ann  \n",
            "  inflating: /content/MACCROBAT2018/28154669.txt  \n",
            "  inflating: /content/MACCROBAT2018/28154700.ann  \n",
            "  inflating: /content/MACCROBAT2018/28154700.txt  \n",
            "  inflating: /content/MACCROBAT2018/28173879.ann  \n",
            "  inflating: /content/MACCROBAT2018/28173879.txt  \n",
            "  inflating: /content/MACCROBAT2018/28190872.ann  \n",
            "  inflating: /content/MACCROBAT2018/28190872.txt  \n",
            "  inflating: /content/MACCROBAT2018/28193213.ann  \n",
            "  inflating: /content/MACCROBAT2018/28193213.txt  \n",
            "  inflating: /content/MACCROBAT2018/28196820.ann  \n",
            "  inflating: /content/MACCROBAT2018/28196820.txt  \n",
            "  inflating: /content/MACCROBAT2018/28202862.ann  \n",
            "  inflating: /content/MACCROBAT2018/28202862.txt  \n",
            "  inflating: /content/MACCROBAT2018/28202865.ann  \n",
            "  inflating: /content/MACCROBAT2018/28202865.txt  \n",
            "  inflating: /content/MACCROBAT2018/28202869.ann  \n",
            "  inflating: /content/MACCROBAT2018/28202869.txt  \n",
            "  inflating: /content/MACCROBAT2018/28207542.ann  \n",
            "  inflating: /content/MACCROBAT2018/28207542.txt  \n",
            "  inflating: /content/MACCROBAT2018/28216610.ann  \n",
            "  inflating: /content/MACCROBAT2018/28216610.txt  \n",
            "  inflating: /content/MACCROBAT2018/28239141.ann  \n",
            "  inflating: /content/MACCROBAT2018/28239141.txt  \n",
            "  inflating: /content/MACCROBAT2018/28248858.ann  \n",
            "  inflating: /content/MACCROBAT2018/28248858.txt  \n",
            "  inflating: /content/MACCROBAT2018/28248891.ann  \n",
            "  inflating: /content/MACCROBAT2018/28248891.txt  \n",
            "  inflating: /content/MACCROBAT2018/28250304.ann  \n",
            "  inflating: /content/MACCROBAT2018/28250304.txt  \n",
            "  inflating: /content/MACCROBAT2018/28250406.ann  \n",
            "  inflating: /content/MACCROBAT2018/28250406.txt  \n",
            "  inflating: /content/MACCROBAT2018/28265107.ann  \n",
            "  inflating: /content/MACCROBAT2018/28265107.txt  \n",
            "  inflating: /content/MACCROBAT2018/28272214.ann  \n",
            "  inflating: /content/MACCROBAT2018/28272214.txt  \n",
            "  inflating: /content/MACCROBAT2018/28272235.ann  \n",
            "  inflating: /content/MACCROBAT2018/28272235.txt  \n",
            "  inflating: /content/MACCROBAT2018/28292056.ann  \n",
            "  inflating: /content/MACCROBAT2018/28292056.txt  \n",
            "  inflating: /content/MACCROBAT2018/28296749.ann  \n",
            "  inflating: /content/MACCROBAT2018/28296749.txt  \n",
            "  inflating: /content/MACCROBAT2018/28296775.ann  \n",
            "  inflating: /content/MACCROBAT2018/28296775.txt  \n",
            "  inflating: /content/MACCROBAT2018/28320420.ann  \n",
            "  inflating: /content/MACCROBAT2018/28320420.txt  \n",
            "  inflating: /content/MACCROBAT2018/28321070.ann  \n",
            "  inflating: /content/MACCROBAT2018/28321070.txt  \n",
            "  inflating: /content/MACCROBAT2018/28321071.ann  \n",
            "  inflating: /content/MACCROBAT2018/28321071.txt  \n",
            "  inflating: /content/MACCROBAT2018/28321073.ann  \n",
            "  inflating: /content/MACCROBAT2018/28321073.txt  \n",
            "  inflating: /content/MACCROBAT2018/28353556.ann  \n",
            "  inflating: /content/MACCROBAT2018/28353556.txt  \n",
            "  inflating: /content/MACCROBAT2018/28353558.ann  \n",
            "  inflating: /content/MACCROBAT2018/28353558.txt  \n",
            "  inflating: /content/MACCROBAT2018/28353561.ann  \n",
            "  inflating: /content/MACCROBAT2018/28353561.txt  \n",
            "  inflating: /content/MACCROBAT2018/28353569.ann  \n",
            "  inflating: /content/MACCROBAT2018/28353569.txt  \n",
            "  inflating: /content/MACCROBAT2018/28353588.ann  \n",
            "  inflating: /content/MACCROBAT2018/28353588.txt  \n",
            "  inflating: /content/MACCROBAT2018/28353596.ann  \n",
            "  inflating: /content/MACCROBAT2018/28353596.txt  \n",
            "  inflating: /content/MACCROBAT2018/28353604.ann  \n",
            "  inflating: /content/MACCROBAT2018/28353604.txt  \n",
            "  inflating: /content/MACCROBAT2018/28353613.ann  \n",
            "  inflating: /content/MACCROBAT2018/28353613.txt  \n",
            "  inflating: /content/MACCROBAT2018/28383413.ann  \n",
            "  inflating: /content/MACCROBAT2018/28383413.txt  \n",
            "  inflating: /content/MACCROBAT2018/28403086.ann  \n",
            "  inflating: /content/MACCROBAT2018/28403086.txt  \n",
            "  inflating: /content/MACCROBAT2018/28403092.ann  \n",
            "  inflating: /content/MACCROBAT2018/28403092.txt  \n",
            "  inflating: /content/MACCROBAT2018/28403099.ann  \n",
            "  inflating: /content/MACCROBAT2018/28403099.txt  \n",
            "  inflating: /content/MACCROBAT2018/28422883.ann  \n",
            "  inflating: /content/MACCROBAT2018/28422883.txt  \n",
            "  inflating: /content/MACCROBAT2018/28538413.ann  \n",
            "  inflating: /content/MACCROBAT2018/28538413.txt  \n",
            "  inflating: /content/MACCROBAT2018/28559815.ann  \n",
            "  inflating: /content/MACCROBAT2018/28559815.txt  \n",
            "  inflating: /content/MACCROBAT2018/28595573.ann  \n",
            "  inflating: /content/MACCROBAT2018/28595573.txt  \n",
            "  inflating: /content/MACCROBAT2018/28767567.ann  \n",
            "  inflating: /content/MACCROBAT2018/28767567.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnUHH8EvtmEl",
        "outputId": "8260f9a7-14b0-4506-fff5-de419d81df72"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.3.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RNKmkWkximol"
      },
      "outputs": [],
      "source": [
        "class Preprocessing_Maccrobat:\n",
        "    def __init__(self, dataset_folder, tokenizer):\n",
        "        self.file_ids = [f.split(\".\")[0] for f in os.listdir(dataset_folder) if f.endswith('.txt')]\n",
        "        self.text_files = [f + \".txt\" for f in self.file_ids]\n",
        "        self.anno_files = [f + \".ann\" for f in self.file_ids]\n",
        "\n",
        "        self.num_samples = len(self.file_ids)\n",
        "\n",
        "        self.texts: List[str] = []\n",
        "        for i in range(self.num_samples):\n",
        "            file_path = os.path.join(dataset_folder, self.text_files[i])\n",
        "            with open(file_path, 'r') as f:\n",
        "                self.texts.append(f.read())\n",
        "\n",
        "        self.tags: List[Dict[str, str]] = []\n",
        "        for i in range(self.num_samples):\n",
        "            file_path = os.path.join(dataset_folder, self.anno_files[i])\n",
        "            with open(file_path, 'r') as f:\n",
        "                text_bound_ann = [t.split(\"\\t\") for t in f.read().split(\"\\n\") if t.startswith(\"T\")]\n",
        "                text_bound_lst = []\n",
        "                for text_b in text_bound_ann:\n",
        "                    label = text_b[1].split(\" \")\n",
        "                    try:\n",
        "                        _ = int(label[1])\n",
        "                        _ = int(label[2])\n",
        "                        tag = {\n",
        "                            \"text\": text_b[-1],\n",
        "                            \"label\": label[0],\n",
        "                            \"start\": label[1],\n",
        "                            \"end\": label[2]\n",
        "                        }\n",
        "                        text_bound_lst.append(tag)\n",
        "                    except:\n",
        "                        pass\n",
        "                self.tags.append(text_bound_lst)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def process(self) -> Tuple[List[List[str]], List[List[str]]]:\n",
        "        input_texts = []\n",
        "        input_labels = []\n",
        "\n",
        "        for idx in range(self.num_samples):\n",
        "            full_text = self.texts[idx]\n",
        "            tags = self.tags[idx]\n",
        "\n",
        "            label_offset = []\n",
        "            continuous_label_offset = []\n",
        "            for tag in tags:\n",
        "                offset = list(range(int(tag[\"start\"]), int(tag[\"end\"]) + 1))\n",
        "                label_offset.append(offset)\n",
        "                continuous_label_offset.extend(offset)\n",
        "\n",
        "            all_offset = list(range(len(full_text)))\n",
        "            zero_offset = [offset for offset in all_offset if offset not in continuous_label_offset]\n",
        "            zero_offset = Preprocessing_Maccrobat.find_continuous_ranges(zero_offset)\n",
        "\n",
        "            self.tokens = []\n",
        "            self.labels = []\n",
        "            self._merge_offset(full_text, tags, zero_offset, label_offset)\n",
        "            assert len(self.tokens) == len(self.labels), f\"Length of tokens and labels are not equal\"\n",
        "\n",
        "            input_texts.append(self.tokens)\n",
        "            input_labels.append(self.labels)\n",
        "\n",
        "        return input_texts, input_labels\n",
        "\n",
        "    def _merge_offset(self, full_text, tags, zero_offset, label_offset):\n",
        "        i = j = 0\n",
        "        while i < len(zero_offset) and j < len(label_offset):\n",
        "            if zero_offset[i][0] < label_offset[j][0]:\n",
        "                self._add_zero(full_text, zero_offset, i)\n",
        "                i += 1\n",
        "            else:\n",
        "                self._add_label(full_text, label_offset, j, tags)\n",
        "                j += 1\n",
        "\n",
        "        while i < len(zero_offset):\n",
        "            self._add_zero(full_text, zero_offset, i)\n",
        "            i += 1\n",
        "\n",
        "        while j < len(label_offset):\n",
        "            self._add_label(full_text, label_offset, j, tags)\n",
        "            j += 1\n",
        "\n",
        "    def _add_zero(self, full_text, offset, index):\n",
        "        start, *_ , end = offset[index] if len(offset[index]) > 1 else (offset[index][0], offset[index][0] + 1)\n",
        "        text = full_text[start: end]\n",
        "        text_tokens = self.tokenizer.tokenize(text)\n",
        "\n",
        "        self.tokens.extend(text_tokens)\n",
        "        self.labels.extend([\"O\"] * len(text_tokens))\n",
        "\n",
        "    def _add_label(self, full_text, offset, index, tags):\n",
        "        start, *_ , end = offset[index] if len(offset[index]) > 1 else (offset[index][0], offset[index][0] + 1)\n",
        "        text = full_text[start:end]\n",
        "        text_tokens = self.tokenizer.tokenize(text)\n",
        "\n",
        "        self.tokens.extend(text_tokens)\n",
        "        self.labels.extend([f\"B-{tags[index]['label']}\"] + [f\"I-{tags[index]['label']}\"] * (len(text_tokens) - 1))\n",
        "\n",
        "    @staticmethod\n",
        "    def build_label2id(tokens: List[List[str]]):\n",
        "        label2id = {}\n",
        "        id_counter = 0\n",
        "        for token in [token for sublist in tokens for token in sublist]:\n",
        "            if token not in label2id:\n",
        "                label2id[token] = id_counter\n",
        "                id_counter += 1\n",
        "        return label2id\n",
        "\n",
        "    @staticmethod\n",
        "    def find_continuous_ranges(data: List[int]):\n",
        "        if not data:\n",
        "            return []\n",
        "        ranges = []\n",
        "        start = data[0]\n",
        "        prev = data[0]\n",
        "        for number in data[1:]:\n",
        "            if number != prev + 1:\n",
        "                ranges.append(list(range(start, prev + 1)))\n",
        "                start = number\n",
        "            prev = number\n",
        "        ranges.append(list(range(start, prev + 1)))\n",
        "        return ranges"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
        "\n",
        "dataset_folder = \"./MACCROBAT2018\"\n",
        "Maccrobat_builder = Preprocessing_Maccrobat(dataset_folder, tokenizer)\n",
        "\n",
        "input_texts, input_labels = Maccrobat_builder.process()\n",
        "label2id = Preprocessing_Maccrobat.build_label2id(input_labels)\n",
        "id2label = {v: k for k, v in label2id.items()}"
      ],
      "metadata": {
        "id": "N1WyIg4Vk7yh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "inputs_train, inputs_val, labels_train, labels_val = train_test_split(\n",
        "    input_texts,\n",
        "    input_labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "RpLbYaSLmLsS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "class NER_Dataset(Dataset):\n",
        "    def __init__(self, input_texts, input_labels, tokenizer, label2id, max_len=MAX_LEN):\n",
        "        super().__init__()\n",
        "        self.tokens = input_texts\n",
        "        self.labels = input_labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label2id = label2id\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_token = self.tokens[idx]\n",
        "        label_token = [self.label2id[label] for label in self.labels[idx]]\n",
        "\n",
        "        input_token = self.tokenizer.convert_tokens_to_ids(input_token)\n",
        "        attention_mask = [1] * len(input_token)\n",
        "\n",
        "        input_ids = self.pad_and_truncate(input_token, pad_id=self.tokenizer.pad_token_id)\n",
        "        labels = self.pad_and_truncate(label_token, pad_id=0)\n",
        "        attention_mask = self.pad_and_truncate(attention_mask, pad_id=0)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.as_tensor(input_ids),\n",
        "            \"labels\": torch.as_tensor(labels),\n",
        "            \"attention_mask\": torch.as_tensor(attention_mask)\n",
        "        }\n",
        "\n",
        "    def pad_and_truncate(self, inputs: List[int], pad_id: int):\n",
        "        if len(inputs) < self.max_len:\n",
        "            padded_inputs = inputs + [pad_id] * (self.max_len - len(inputs))\n",
        "        else:\n",
        "            padded_inputs = inputs[:self.max_len]\n",
        "        return padded_inputs\n",
        "\n",
        "    def label2id(self, labels: List[str]):\n",
        "        return [self.label2id[label] for label in labels]\n",
        "\n",
        "train_set = NER_Dataset(inputs_train, labels_train, tokenizer, label2id)\n",
        "val_set = NER_Dataset(inputs_val, labels_val, tokenizer, label2id)"
      ],
      "metadata": {
        "id": "54mZo0_pos6h"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"d4data/biomedical-ner-all\",\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    ignore_mismatched_sizes=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbKt8AGjsifb",
        "outputId": "c2b6ae99-812b-4a51-a963-e5e2056e0301"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at d4data/biomedical-ner-all and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([84]) in the checkpoint and torch.Size([83]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([84, 768]) in the checkpoint and torch.Size([83, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    mask = labels != 0\n",
        "    predictions = np.argmax(predictions, axis=-1)\n",
        "    return accuracy.compute(predictions=predictions[mask], references=labels[mask])"
      ],
      "metadata": {
        "id": "PoXpgnX8tYCh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"out_dir\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=20,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    optim=\"adamw_torch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=val_set,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "5VLu4LZxtf1J",
        "outputId": "367d3dc5-221d-409b-9ca9-55b2ce2c62b9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-c0a8e7295acb>:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 04:44, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.837491</td>\n",
              "      <td>0.269934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.126441</td>\n",
              "      <td>0.564535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.841218</td>\n",
              "      <td>0.672924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.709208</td>\n",
              "      <td>0.741777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.666900</td>\n",
              "      <td>0.748505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.647028</td>\n",
              "      <td>0.761960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.643591</td>\n",
              "      <td>0.759884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.649897</td>\n",
              "      <td>0.770681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.656761</td>\n",
              "      <td>0.771678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.646651</td>\n",
              "      <td>0.771927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.664091</td>\n",
              "      <td>0.776329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.660949</td>\n",
              "      <td>0.776993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.669081</td>\n",
              "      <td>0.776246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.665372</td>\n",
              "      <td>0.778904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.671212</td>\n",
              "      <td>0.781312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.673338</td>\n",
              "      <td>0.782060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.678708</td>\n",
              "      <td>0.781561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.678110</td>\n",
              "      <td>0.781229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.677333</td>\n",
              "      <td>0.780482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.676956</td>\n",
              "      <td>0.781312</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=0.42422473907470704, metrics={'train_runtime': 286.1274, 'train_samples_per_second': 11.184, 'train_steps_per_second': 0.699, 'total_flos': 418702245888000.0, 'train_loss': 0.42422473907470704, 'epoch': 20.0})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"\"\"A 48 year-old female presented with vaginal bleeding and abnormal\n",
        "Pap smears. Upon diagnosis of invasive non-keratinizing SCC of the cervix, she\n",
        "underwent a radical hysterectomy with salpingo-oophorectomy which demonstrated\n",
        "positive spread to the pelvic lymph nodes and the parametrium. Pathological\n",
        "examination revealed that the tumour also extensively involved the lower uterine\n",
        "segment.\"\"\"\n",
        "\n",
        "# Tokenization\n",
        "input_ids = torch.as_tensor([tokenizer.convert_tokens_to_ids(test_sentence.split())])\n",
        "\n",
        "# Move to CUDA (if using GPU)\n",
        "input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "# Prediction\n",
        "outputs = model(input_ids)\n",
        "_, preds = torch.max(outputs.logits, -1)\n",
        "preds = preds[0].cpu().numpy()\n",
        "\n",
        "# Decode predictions\n",
        "for token, pred in zip(test_sentence.split(), preds):\n",
        "    print(f\"{token}\\t{id2label[pred]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZv-uENEt1vT",
        "outputId": "78c888da-4189-4262-c5ce-9567b4c209c0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\tO\n",
            "48\tO\n",
            "year-old\tO\n",
            "female\tO\n",
            "presented\tO\n",
            "with\tO\n",
            "vaginal\tO\n",
            "bleeding\tB-Sign_symptom\n",
            "and\tO\n",
            "abnormal\tO\n",
            "Pap\tO\n",
            "smears.\tO\n",
            "Upon\tO\n",
            "diagnosis\tO\n",
            "of\tO\n",
            "invasive\tO\n",
            "non-keratinizing\tO\n",
            "SCC\tO\n",
            "of\tO\n",
            "the\tO\n",
            "cervix,\tO\n",
            "she\tO\n",
            "underwent\tO\n",
            "a\tO\n",
            "radical\tO\n",
            "hysterectomy\tO\n",
            "with\tO\n",
            "salpingo-oophorectomy\tO\n",
            "which\tO\n",
            "demonstrated\tO\n",
            "positive\tB-Lab_value\n",
            "spread\tO\n",
            "to\tO\n",
            "the\tO\n",
            "pelvic\tO\n",
            "lymph\tI-Biological_structure\n",
            "nodes\tO\n",
            "and\tO\n",
            "the\tO\n",
            "parametrium.\tO\n",
            "Pathological\tO\n",
            "examination\tO\n",
            "revealed\tO\n",
            "that\tO\n",
            "the\tO\n",
            "tumour\tO\n",
            "also\tO\n",
            "extensively\tO\n",
            "involved\tO\n",
            "the\tO\n",
            "lower\tO\n",
            "uterine\tO\n",
            "segment.\tO\n"
          ]
        }
      ]
    }
  ]
}